# 区分长期和短期兴趣进行推荐--论文精读

#### Q：为了什么而做

建模用户的长期和短期兴趣对于准确推荐至关重要。然而，由于没有人工标注的用户兴趣标签，现有的方法总是遵循将这两个方面纠缠在一起的范式，这可能导致推荐精度和可解释性较差。

#### Q：有哪些挑战

首先，LS - term兴趣反映了用户偏好的不同方面。具体来说，长期兴趣可以看作是用户的整体偏好，可以在较长时间内保持稳定，而短期兴趣则表示用户的动态偏好，根据最近的交互而快速演化。因此，学习LS - term兴趣的统一表示不足以捕获这种差异。**相反，对这两个方面分别建模更为合适。（分开建模）**

第二，用于学习LS - term兴趣的标注数据难以获得。采集到的行为日志数据往往只包含用户的点击等隐式反馈。**因此，LS -长期利益的单独建模对于区分这两个方面缺乏明确的监督。**

最后，对于用户未来交互行为的最终预测，需要同时考虑用户的长期和短期兴趣。然而，在不同的用户-项目交互中，两种兴趣的重要性是不同的。例如，当用户连续浏览相似的项目时，短期兴趣更为重要，而当用户切换到差异较大的项目时，其行为在很大程度上受到长期兴趣的驱动。**因此，自适应地融合这两个方面对于预测未来的交互是关键但具有挑战性的。**

#### Q：做了什么？

在本文中，为了解决这个问题，我们提出了一个对比学习框架，通过自我监督来解耦推荐的长期和短期兴趣( CLSR )。

具体来说，我们首先提出了两个独立的编码器来独立地捕获不同时间尺度的用户兴趣。（第一个挑战）然后，我们从交互序列中提取长期和短期兴趣代理，作为用户兴趣的伪标签。然后设计两两对比任务进行监督（第二个挑战）

最后，由于长期和短期利益的重要性是动态变化的，我们提出通过基于注意力的网络自适应地将它们聚合在一起进行预测。（第三个挑战）

#### Q：之前的技术是怎么做的

Yu等人[ 47 ]为短期利益开发了LSTM的变体，为长期利益采用非对称SVD [ 22 ]。然而，由于这些方法对学习到的利益表征不加监督，因此无法保证LS -长期利益的解缠。

例如，Zhou等[ 52 ]开发了一种基于互信息最大化的自监督序列推荐器。和Ma等人[ 32 ]提出用潜在意图原型来监督序列编码器。然而，这些方法忽略了长期兴趣和短期兴趣之间的差异，而这些差异对于准确推荐至关重要

Ma等人[ 31 ]提出基于变分自编码器学习用户的多种偏好。Wang等人[ 42 ]利用知识图谱来学习不同的用户意图，并将它们规则化为彼此不同的。然而，这些工作大多由于缺乏有标签的数据，即无监督的解纠缠，而未能对学习到的多重表示施加特定的语义，这已被证明是无效的[ 28 ]。

#### Q：核心贡献

我们强调了用户长期和短期兴趣的不同动态，并采取先驱性的步骤来解开这两个因素对于准确推荐至关重要。

我们提出了一个对比学习框架来单独捕获LS - term兴趣。解耦的表示被学习到通过与从原始交互序列构造的代理表示进行比较来进行自我监督。进一步设计了基于注意力机制的融合网络，自适应地聚合LS - term兴趣来预测交互。

我们在真实的数据集上进行了广泛的实验。实验结果验证了我们提出的CLSR相对于SOTA方法取得了显著的改进。进一步的反事实分析表明，CLSR可以实现对长期利率的更强的解耦

#### Q：具体实现方法

##### 问题定义：

输入：所有用户的历史交互序列{ xu } M u = 1。

输出：一个预测模型，它估计用户是否会点击一个项目的概率，同时考虑LS项的兴趣。

**3.1用户兴趣建模**

![image-20240126160954074](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126160954074.png)

其中，f1、f2、f3分别为用户U的长期利益( Ul )、短期利益( U ( t ) s )以及与物品V ( t )的交互( Y ( t ) )的底层函数。当前时间戳和最后时间戳分别记为t和t - 1。值得注意的是，U表示用户配置文件，包含用户ID和交互历史xu。

<img src="C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126162323081.png" alt="image-20240126162323081" style="zoom:200%;" />

图1：用户兴趣建模ζ (在色彩上最好看)，包括三种机制，即长期兴趣表示(红边)、短期兴趣演化(蓝边)和交互预测(黄边)。

( 1 )式表示长期利益。长期兴趣反映了用户偏好的整体观，因此**它是稳定的，受近期交互的影响较小**。也就是说，**长远利益可以从整个历史交互序列中推断出来，因此我们将U作为f1的输入，其中包含了交互历史xu。**

式( 2 )中的短期利益演化。随着用户不断与推荐项目进行交互，短期兴趣也在不断演化[ 50 ]。例如，用户在点击一个项目后可能会建立新的兴趣。同时，用户也可能逐渐失去一定的兴趣。也就是说，**短期兴趣是与时间有关的变量，因此在f2中，在时间戳t处的短期兴趣U ( t ) s是由U ( t-1 ) s递归演化而来的，受到最后一个与项V ( t-1 )相互作用的Y t - 1的影响.**

式( 3 )中的交互作用预测。在预测未来的互动时，长期或短期利益是否发挥更重要的作用取决于各种各样的方面，包括目标项V ( t )和U的互动历史xu [ 47 ]。因此，**我们根据V ( t )和U自适应地融合Ul和U ( t ) s，以准确预测相互作用。**



**3.2 自我监督实施**

1.为Ls - Term兴趣生成查询向量。

设计了两个单独的**注意力编码器φ和ψ**，分别捕获这两个方面。首先，我们为LS - term兴趣生成如下查询向量：

![image-20240126190841356](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126190841356.png)

其中，我们使用一个**查找嵌入表和一个门控循环单元(** Gated Recurrent Unit，GRU ) [ 9 ]来捕捉随时间变化的不同动态。为了**对嵌入相似度施加额外的自我监督**，所有的嵌入都需要在同一个语义空间中。因此，我们使用项目的历史序列作为注意力编码器的键，从而得到的LS - term兴趣表示在相同的物品嵌入空间如下，

![image-20240126191037890](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126191037890.png)

其中ut l和ut s是LS项兴趣的学习表示。



2.长期兴趣Encoder

![image-20240126191220638](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126191220638.png)

图2：我们提出的基于自监督学习的CLSR框架。A )关于LS -词项兴趣的表征和代理之间的相似性的对比任务，以增强解缠；B )长期利益编码器φ；C )短期利益编码器ψ；D ) LSterm兴趣项与目标项注意力和历史交互项的自适应融合；E )交互预测网络。

我们使用注意力池化来提取长期兴趣表示，每个项目xj u的注意力得分可以如下计算。

![image-20240126191250983](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126191250983.png)

其中Wl是一个变换矩阵，τ l是一个多层感知器( MLP )网络，∥表示嵌入的级联**.最终学习到的长期利益表征是整个交互历史的加权聚合**，**权重由**上述注意力网络计算得到，公式如下：

![image-20240126191341945](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126191341945.png)

3.短期兴趣Encoder。

![image-20240126191220638](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126191220638.png)

我们在循环神经网络( RNN )的基础上使用了另一个注意力网络。具体来说，我们将历史项嵌入反馈到一个RNN模型中，并将RNN的输出作为密钥，其表达式如下：

![image-20240126191558357](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126191558357.png)

其中Ws为变换矩阵，ρ表示RNN模型。我们使用qu，t s作为查询向量，得到注意力分数bk。然后，对于短期兴趣的学习表示可以如下计算

![image-20240126191712417](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126191712417.png)

4.Ls - -team兴趣的自我监督解脱

具体来说，我们计算了整个互动历史的平均表示作为长期利益的代理，并使用最近k次互动的平均表示作为短期利益的代理。形式上，给定用户u在时间戳t的LS -长期利益代理可以计算如下：

![image-20240126192024128](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126192024128.png)

式中：E ( x )表示项x的嵌入。在我们的方法中，阈值lt，最近行为序列的长度k是超参数。此外，我们在这里使用均值池化，因为它很简单，性能也足够好。事实上，我们的自监督范式能够利用我们留给未来工作的代理的更复杂的设计。

![image-20240126191220638](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126191220638.png)

我们可以利用它们来监督LS -长期利益的解缠。具体来说，我们在编码器输出和代理之间进行对比学习，这要求学习到的LS - term兴趣表示与它们对应的代理比相反的代理更相似。我们对图2 ( A )中的对比任务进行说明。从形式上看，有以下四个对比任务。

![image-20240126192302087](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126192302087.png)

其中Eqn ( 19 ) - ( 20 )监督长期利益，Eqn ( 21 ) - ( 22 )监督短期利益，sim( · , ·)度量嵌入相似性。以长期利益建模为例，Eqn ( 19 )鼓励学习到的长期利益表示ut l更类似于长期代理pu，t l，而不是短期代理pu，t s。同时，式( 20 )要求ut l比短期利益表示ut s更接近pu，t l。通过四个关于编码器输出和代理之间相似性的对称对比任务，我们在LS - term兴趣建模上增加了自我监督，与现有的无监督方法相比，可以实现更强的去纠缠。

​	我们在Eqn ( 19 ) - ( 22 )中实现了两个基于贝叶斯个性化排序( BPR ) [ 35 ]和三元组损失的**成对损失函数**来完成对比学习。在形式上，使用**内积和欧氏距离**来捕获嵌入相似性的两个损失函数计算如下。

![image-20240126192915337](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126192915337.png)

式中：σ为softplus激活函数，⟨·，· · ·为两个嵌入的内积，d为欧氏距离，m为一个正的边距值。Lbpr和Ltr i的设计都是为了使锚比负样本q更类似于正样本p。因此，LS -项兴趣的自监督解纠缠的对比损失可以如下计算。

![image-20240126192940657](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126192940657.png)

这里我们省略了兴趣表示和代理的上标，f可以是Lbpr或者Ltr i .

总之，我们实现了两个独立的编码器φ和ψ，分别用于学习LS项兴趣的表示。为了实现LS -长期利益的解纠缠，我们从历史交互序列中计算代理。我们进一步提出了对比学习损失函数，以自监督的方式引导两个编码器仅捕获期望的方面。



5.自适应融合进行交互预测。

事实上，无论是长期还是短期，更重要的取决于历史的先后顺序。例如，用户在连续浏览同一类别的物品时，主要受到短期兴趣的驱使。同时，LS -长期利益的重要性还取决于目标项。例如，一个运动爱好者即使在浏览了几本书之后，也可能会因为长期的兴趣而点击推荐的自行车。因此，我们将历史序列和目标项同时作为聚合器的输入，其中历史序列用一个GRU进行压缩。本文提出的基于注意力的自适应融合模型如图2 ( D )所示，该模型动态地确定LS - term兴趣对聚合ut l和ut s的重要性。形式上，得到最终的融合利益为：

![image-20240126193253356](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126193253356.png)

式中：σ为sigmoid激活函数，τ f为用于融合的MLP。这里α表示基于历史交互、目标项目和用户LS - term兴趣的估计融合权重。

![image-20240126191220638](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126191220638.png)

为了预测相互作用，我们使用了广泛采用的两层MLP [ 47 ]，如图2 ( E )所示。然后给出用户u和物品v在时间戳t + 1估计分数可以预测如下

![image-20240126193446884](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126193446884.png)

沿用已有工作[ 47 ]的设置，我们使用负对数似然损失函数如下。

![image-20240126193512362](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126193512362.png)

其中O是由一个正样本xu t + 1和N - 1个负样本组成的训练对构成的集合.我们以端到端的方式在两个目标上进行多任务学习来训练模型。具体来说，具有超参数β的平衡目标的联合损失函数可以表述为：

![image-20240126193544984](C:\Users\86157\AppData\Roaming\Typora\typora-user-images\image-20240126193544984.png)

其中λ∥Θ∥2表示用于解决过拟合的L2正则化.我们的实现的计算复杂度为O ( ( M + N ) d + Q )，其中Q表示MLP和GRU的复杂度，这与最先进的SLi - Rec方法相当[ 47 ]。